% !TEX encoding = UTF-8 Unicode
% !TEX TS-program = XeLaTeX
% !TEX spellcheck = English
% !TEX pdfSinglePage



\documentclass[aspectratio=1610,14pt]{beamer}
	\makeatletter
	\overfullrule1em
	\beamertemplatenavigationsymbolsempty
	\setbeamercovered{transparent}
	\setbeamersize{text margin left=3mm,text margin right=3mm}
	\def\CMH{\gdef\beamer@currentmode{handout}}
	\def\CMB{\gdef\beamer@currentmode{beamer}}
	
	\catcode`æ¿€13 \defæ¿€#1{\lccode`~`#1\lowercase{\catcode`#113\def~}}
	æ¿€è‰²#1!#2 {\definecolor{#1}{HTML}{#2}} % https://en.wikipedia.org/wiki/National_Taiwan_University#/media/File:National_Taiwan_University_seal.svg
	è‰²NTU Maroon!661E2A	è‰²NTU Gold!B59D15	è‰²NTU yellow!FDE8AA	è‰²NTU green!B59D15	
	\setbeamercolor{normal text}{bg=white,fg=black}
	\setbeamercolor{structure}{fg=NTU Maroon}
	\setbeamercolor{alerted text}{fg=NTU Gold}
	\setbeamercolor{example text}{fg=NTU yellow}

\usepackage{xurl}
	\hypersetup{pdfsubject=Information Theory (cs.IT)}
	\hypersetup{colorlinks,linkcolor=NTU Maroon,citecolor=NTU Gold,urlcolor=NTU green}

\usepackage{mathtools}

\usepackage[warnings-off={mathtools-colon,mathtools-overbracket}]{unicode-math}
	\setmainfont{Times}
	\setsansfont{HelveticaNeue-Light}
	\setmonofont{Menlo}
	\setmathfont[sans-style=literal]{texgyrepagella-math.otf}
	
	\DeclareMathOperator*\argmax{argmax}
	\DeclareMathOperator*\mybest{do-my-best}
	\DeclareMathOperator*\wt{wt}
	\def\P{P_\mathrm e}
	
	\def\({\bigl(}	\def\){\bigr)}	æ¿€ï¼ˆ{\Bigl(}		æ¿€ï¼‰{\Bigr)}		
	æ¿€ï¼»{\bigl[}		æ¿€ï¼½{\bigr]}		æ¿€ã€Œ{\Bigl[}		æ¿€ã€{\Bigr]}		
	æ¿€ï½›{\bigl\{}	æ¿€ï½{\bigr\}}	æ¿€ã€{\Bigl\{}	æ¿€ã€{\Bigr\}}	
	æ¿€ã€{\lvert}		æ¿€ã€‘{\rvert}	
	
	æ¿€ã’{\log}		æ¿€ã‘{\ln}		
	æ¿€Ë†{\hat}		æ¿€Â¯{\bar}		
	æ¿€ï½œ{\mid}		æ¿€ï¼š{\colon}		æ¿€ï¼›{\mathrel;\nobreak}		
	æ¿€Ã·{\frac}		æ¿€âˆš{\sqrt}		æ¿€Â¬{\limits}		
	æ¿€â€ #1â€ {{\text{#1}}}				
	æ¿€â‹†{\raisebox{1ex}{$\star$}}
	
	\def\pp{\pause\par}
	\advance\parskip\fill

\usepackage{tikz,tikz-cd}
	\let\PMP\pgfmathparse\def\PMR{\pgfmathresult}
	\let\PMS\pgfmathsetmacro\let\PMT\pgfmathtruncatemacro\let\PMD\pgfmathdeclarefunction
	% https://tex.stackexchange.com/q/420034/
	\PMD*{axis_height}0{\begingroup\pgfmathreturn.25em\endgroup}
	\PMD*{rule_thickness}0{\begingroup\pgfmathreturn.06em\endgroup}
	\tikzset{
		every picture/.style={cap=round,join=round,line width=rule_thickness},
		% https://tex.stackexchange.com/q/146908/
		alt/.code args={<#1>#2#3}{\alt<#1>{\pgfkeysalso{#2}}{\pgfkeysalso{#3}}},
		uncover/.style={alt=<#1>{}{opacity=.15}},
	}

\usepackage{pgfplotstable,booktabs,colortbl}
	\pgfplotsset{compat/.cd,show suggested version=false,=1.17}
	\pgfplotstableset{
		every head row/.style={before row=\toprule,after row=\midrule},
		every last row/.style={after row=\bottomrule},string type,
	}
	\def\arraystretch{1.5}

\usepackage{pst-barcode}



\title[Limits of Communication \& How to Achieve]
	{The Fundamental Limits of Communication and How to Achieve Them}
\author[H-P\ Wang]{Hsin-Po WANG}
\institute{Department of Mathematics, University of Illinois at Urbana--Champaign}
\date[2021-01]{2021-01-04 NTU Special Math Seminar}

\begin{document}

\frame\maketitle

\makeatletter\defbeamertemplate*{sidebar right}{thinbold}{
	\tikz[remember picture,overlay,x=3mm,y=\paperheight]{\footnotesize
		\PMS\olfrac{\insertoverlaynumber/(\insertframeendpage+1-\insertframestartpage)}
		\path[save path=\stare,yscale={1/max(\insertmainframenumber-1,1)}]
			(-1,0)-|(0,1-\insertframenumber)-|+(-\olfrac,1)-|cycle;
		\tikzset{banner/.pic={\node at(-.55,-.5)[rotate=-90]
			{\beamer@shorttitle~\beamer@shortdate~\beamer@shortauthor};}}
		\fill[use path=\stare,gray!50]pic[gray]{banner};
		\clip[use path=\stare]pic[white]{banner};
	}
}

%\CMH

\frame{{1940s; Bell labs; Model V computer}
	$$\includegraphics[width=10em]{ModelV.jpg}$$
	\pp
	Should the machine detect an error, it pauses.	\\
	A person will come to fix it.	\\
	That person does not work on weekends.
}

\frame{{1940s; Bell labs; Model V computer}
	Richard W.\ Hamming (UIUC PhD 1942) usually ordered the machine	\\
	to do complicated computation over the weekend,	\\
	only to find on Monday that it was interrupted on Friday midnight.
	\pp
	Quote from an interview: \hfill{\tiny[ISBN:0883850370 p.17]}	\\
	\itshape Damn it, if the machine can detect an error,	\\
	why can't it locate the position of the error and \alert{correct} it?
}

\frame{{Hamming code}
	For every 7 bits, instead of all 128 vectors, allow only the null space of
	$$H=\begin{bmatrix}
		1	&	1	&	1	&	1	&	0	&	0	&	0	\\
		1	&	1	&	0	&	0	&	1	&	1	&	0	\\
		1	&	0	&	1	&	0	&	1	&	0	&	1
	\end{bmatrix}.$$
	\pp
	Let $Â¯xâˆˆğ”½_2^7$ be a valid vector (codeword), that is, $HÂ¯x=0$.	\\
	If $Â¯y=Â¯x+Â¯e$ is the corrupted word, where $Â¯e$ contains one $1$ and six $0$'s,	\\
	then $HÂ¯y=H(Â¯x+Â¯e)=HÂ¯e$ is the error position; flip that bit back.
}

\frame{{Performance of Hamming code}
	Block length $N=7$ (how many bits are grouped together).
	\pp
	Code rate $R=4/7$ (how efficiently the information is recorded).
	\pp
	What could go wrong? When $Â¯e$ contains more than one $1$.	\\
	How often will things go wrong?
}
\frame{{Probabilistic model}
	Input is a random variable $Xâˆˆğ”½_2$; output is another $Yâˆˆğ”½_2$.	\\
	$Y$ is usually $X$; But $Y$ could be $1-X$ (flipped) with probability $p$.
	\pp
	$$\tikz{
		\draw
			(-3,2)node(a){\Large$1$}(3,2)node(+){\Large$1$}
			(-3,0)node(b){\Large$0$}(3,0)node(-){\Large$0$}
			(a)edge[->]node[above]{w.p.\ $1-p$}(+)
			   edge[->]node[above,sloped,pos=.8]{w.p.\ $p$}(-)
			(b)edge[->]node[below]{w.p.\ $1-p$}(-)
			   edge[->]node[above,sloped,pos=.2]{w.p.\ $p$}(+)
		;
	}$$
	Binary symmetric channel (BSC) with crossover probability $p$.
}

\frame{{Another probabilistic model}
	Binary erasure channel (BEC) with erasure probability $p$.	\\
	$Y$ is usually $X$; But $Y$ could be ``$?$'' (erased) with probability $p$.
	$$\tikz[y=1cm]{
		\draw
			(-3,2)node(a){\Large$1$}(3,2)node(+){\Large$1$}
									(3,1)node(?){\Large$?$}
			(-3,0)node(b){\Large$0$}(3,0)node(-){\Large$0$}
			(a)edge[->]node[above]{w.p.\ $1-p$}(+)
			   edge[->]node[below,sloped,pos=.25]{w.p.\ $p$}(?)
			(b)edge[->]node[below]{w.p.\ $1-p$}(-)
			   edge[->]node[above,sloped,pos=.25]{w.p.\ $p$}(?)
		;
	}$$
	\pp
	In classical coding theory, BSC and BEC are the mental models.
}

\frame{{Coding $=$ encoding $+$ decoding}
	Choose a \alert{codebook} $â„¬âŠ†ğ”½_2^N$, the set of valid codewords.	\\
	The inputs are limited to codewords in the codebook $Â¯X_1^Nâˆˆâ„¬$.	\\
	$Â¯X_1^N$ is shorthand for a vector $X_1X_2\dotsm X_Nâˆˆğ”½_2^N$.
	\pp
	Example: $â„¬â‰”\{0000,0111,1100\}âŠ†ğ”½_2^3$ over BEC.	\\
	See $0?00$ $âŸ¹$ it must be $0000$ with one erasure.	\\\pause
	See $1?0?$ $âŸ¹$ it must be $1100$ with two erasures.	\\\pause
	See $??00$ $âŸ¹$ No idea; could be $0000$ or $1100$.
}

\frame{{More ambiguity}
	Example: $â„¬â‰”\{0000,0111,1100\}âŠ†ğ”½_2^3$ over BSC.	\\
	See $0001$ $âŸ¹$ it is most likely $0000$, likelihood $p(1-p)^3$;	\\\pause
	$âŸ¹$ but could be $0111$ with two flips, likelihood $p^2(1-p)^2$;	\\\pause
	$âŸ¹$ could also be $1100$ with \alert{three} flips, likelihood $p^3(1-p)$.
	\pp
	See $Y_1Y_2Y_3Y_4$ $âŸ¹$ compute the likelihood for $x_1x_2x_3x_4âˆˆâ„¬$ by	\\
	$â„™(Y_1Y_2Y_3Y_4ï½œx_1x_2x_3x_4)=â„™(Y_1ï½œx_1)â„™(Y_2ï½œx_2)â„™(Y_3ï½œx_3)â„™(Y_4ï½œx_4)$,	\\
	where $â„™(Y_iï½œx_i)$ equals $1-p$ if $Y_1=x_1$, or $p$ otherwise.
	\pp
	Find the $x_1x_2x_3x_4âˆˆâ„¬$ that maximizes the likelihood.
}

\frame{{Goal of coding}
	A good codebook $â„¬$ should possess the following properties.
	\pp
	Small \alert{block length} $N$ $âŸ¹$ recall $â„¬âŠ†ğ”½_2^N$; shorter vectors, easier life.
	\pp
	Big $|â„¬|$ $âŸ¹$ more codewords mean sending more information at once.	\\
	In fact, it is $Râ‰”\dfrac{\log_2|â„¬|}N$ that should be large;
	this is the \alert{code rate}.
	\pp
	Low \alert{error probability} $P$ $âŸ¹$ efforts are pointless if information is lost.
}

\frame{{Fundamental limit of coding}
	Consider the parameter triple $(N,R,P)$	\\
	(block length, code rate, error probability).
	\pp
	Shannon48:
	You can find a series of block codes $â„¬_1,â„¬_2,â„¬_3,\dotsc$	\\
	such that $N(â„¬_n)â†’âˆ$ and $R(â„¬_n)â†’C$ and $P(â„¬_n)â†’0$ as $nâ†’âˆ$,	\\
	where $C$ is a magic number that you cannot beat, provably.
	\pp
	$C$ is called the \alert{Shannon capacity}. It is the fundamental limit of coding.
}

\frame{{How to construct codebook}
	Let $N$ be really large. Let $Râ‰”C-Îµ$. We preset $|â„¬|=2^{RN}$.
	\pp
	Choose a random subset $â„¬âŠ†ğ”½_2^{N}$ of that size. (It's just that easy!)
	\pp
	With high probability, any two distinct codewords in $â„¬$ are	\\
	\alert{far away enough} such that you cannot (well$âˆ¼$) confuse them.
}

\frame{{What Shannon doesn't tell you}
	Of course we can make $P$ (or $R$) arbitrarily close to $0$ (or $C$).	\\
	But at what cost? If $Nâ‰¥10^{100}$ is needed, the whole theory is useless.
	\pp
	Next step: understand the relation between $N$ and $P$ and $C-R$.
}

\frame{{The trend of $P$}
	Fix a code rate $R<C$, error probability $P$ decays to $0$ really fast:
	$$Pâ‰ˆe^{-Î˜(N)}.$$
	\pp
	To get a feeling: $P=10^{-3}$ (once per hour) at $N=100$,	\\\pause
	$P=10^{-6}$ (once per day) at $N=200$.	\\\pause
	$P=10^{-9}$ (once per year) at $N=300$.	\\\pause
	$P=10^{-12}$ (once per lifetime) at $N=400$.
}

\frame{{The trend of $R$}
	Fix an error probability $P$, code rate $R$ approaches $C$ moderately fast:
	$$Râ‰ˆC-Ã·1{âˆšN}.$$
	\pp
	$R=C-1/10$ at $N=100$ (CPU word).	\\\pause
	$R=C-1/100$ at $N=10^4$ (internet package).	\\\pause
	$R=C-1/1000$ at $N=10^6$ (least file size of linux).
}

\frame{{The joint trend of $P$ and $R$}
	The trend of $P$ says $Pâ‰ˆe^{-Î˜(N)}$; or equivalently $-\log Pâ‰ˆN$.	\\
	The trend of $R$ says $C-Râ‰ˆ1/âˆšN$; or equivalently $1/(C-R)^2â‰ˆN$.
	\pp
	A (natural) generalization
	$$Ã·{-\log P}{(C-R)^2}â‰ˆN.$$
}

\frame{{Analog in probability theory}
	Probability theory concerns sum of i.i.d.---$Z$ mean $Î¼$ variance $Ïƒ$.
	\pp
	Large deviations principle:	\\
	Fix a $z$, then $â„™ã€Z_1+Z_2+\dotsb+Z_N>N(Î¼+z)ã€=e^{-Î˜(N)}$.
	\pp
	Central limit theory:	\\
	$Z_1+Z_2+\dotsb+Z_Nâ‰ˆâ€ Normalâ€ \(Î¼,ÏƒâˆšN\)$; or equivalently $Ã·{âˆ‘Z}Nâ‰ˆÎ¼Â±Ã·{Î˜(1)}{âˆšN}$.
	\pp
	Generalization: moderate deviations principle:	\\
	$Ã·{-\logâ„™ï½›Z_1+Z_2+\dotsb+Z_N>N(Î¼+Îµ_nz)ï½}{Îµ_n^2}â‰ˆN$.
}

\frame{{Comparison}
	\centering
	\begin{tabular}{ccc}
		\toprule
			&Probability theory						&Coding theory					\\
		\midrule
		LLN	&$Â¯Zâ†’Î¼$									&$(P,R)â†’(0,C)$					\\
		LDP	&$â„™\{Â¯Z-Î¼>z\}â‰ˆe^{-NI(z)}$				&$Pâ‰ˆe^{-E_â€ râ€ (R)N}$				\\
		CLT	&$Â¯Z-Î¼âˆ¼â€ Normalâ€ \(0,Ã·Ïƒ{âˆšN}\)$			&$C-Râ‰ˆÃ·{Q^{-1}(P)}{âˆš{VN}}$		\\
		MDP	&$Ã·{-\logâ„™\{Â¯Z-Î¼>Îµ_Nz\}}{Îµ_N^2}â‰ˆNI(z)$	&$Ã·{-\log P}{(C-R)^2}â‰ˆÃ·N{2V}$	\\
		\bottomrule
	\end{tabular}
}

\frame{{What second-order theory doesn't tell you}
	Of course we can make $P$ (or $R$) close to $0$ (or $C$)	\\
	at the pace of $Ã·{-\log P}{(C-R)^2}â‰ˆÃ·N{2V}$, but at what cost?
	\pp
	Recall that we have to compute likelihoods (assuming i.i.d.\ channels)	\\
	$â„™(Y_1Y_2\dotsm Y_Nï½œx_1x_2\dotsm x_N)=â„™(Y_1ï½œx_1)â„™(Y_2ï½œx_2)\dotsmâ„™(Y_Nï½œx_N)$	\\
	and choose the maximizing $x_1x_2x_3x_4âˆˆâ„¬$.
	\pp
	Next step: how to maximize likelihood, \alert{easily}?
}

\frame{{Low complexity codes}
	Engineers develop ``easy codes'' regardless of Shannon theory.
	\pp
	Reed--Muller (1954),
	convolutional (1955),
	Bose--Chaudhuri--Hocquenghem (1959),
	Reed--Solomon (1960),
	trellis modulation (1970s),
	turbo (1990s),
	low-density parity-check (1963 and 1996),
	repeat-accumulate (1998),
	fountain (1998),
	and polar (2009).
	\pp
	Only polar codes and LDPC codes achieve first-order limit.
}

\frame{{Who achieves second-order limits?}
	\centering
	\begin{tabular}{ccc}
		\toprule
			&Coding theory					&Years								\\
		\midrule
		LLN	&$(P,R)â†’(0,C)$					&random 1948, polar 2009, LDPC 2014	\\
		LDP	&$Pâ‰ˆe^{-E_â€ râ€ (R)N}$				&random 1961, polar 2009--14		\\
		CLT	&$C-Râ‰ˆÃ·{Q^{-1}(P)}{âˆš{VN}}$		&random 1950, polar 2010--20		\\
		MDP	&$Ã·{-\log P}{(C-R)^2}â‰ˆÃ·N{2V}$	&random 2014, polar 2016--20		\\
		\bottomrule
	\end{tabular}
}

\frame{{Polar codes key ideas}
	Consider BEC with erasure probability $0<p<1$.
	\pp
	Polar transformation $pâ†¦(p^2,2p-p^2)$, recursively.	\\
	$p^2â†¦(p^4,2p^2-p^4)$ and $2p-p^2â†¦\((2p-p^2)^2,2(2p-p^2)-(2p-p^2)^2\)$.
	\pp
	ArÄ±kan's martingale $Z_{n+1}=Z_n^2$ or $2Z_n-Z_n^2$ with equal probability.
	\pp
	Doob's martingale convergence theorem $Z_nâ†’Z_âˆâˆˆ\{0,1\}$.	\\
	But how fast? (If it takes forever to converge the theory is useless.)
}

\frame{{Polar counterpart of LDP}
	When $p$ is really really small, $pâ†¦(p^2,p)$. That is, $\log pâ†¦(2\log p,\log p)$,	\\
	or $\log_2(-\log p)â†¦\(1+\log_2(-\log p),\log_2(-\log p)\)$.
	\pp
	With $1/2$ probability, $\log_2(-\log Z_n)$ increases by $1$, otherwise nothing.	\\
	For those $Z_n$ that are small, $\log_2(-\log Z_n)â‰ˆn/2$; or $Z_nâ‰ˆe^{-2^{n/2}}$.
	\pp
	With more advanced tricks, $Z_nâ‰ˆe^{-â„“^{0.99n}}$; this means $Pâ‰ˆe^{-N^{0.99}}$.
}

\frame{{Polar counterpart of CLT}
	$âˆš{Z_n(1-Z_n)}$ is a supermartingale. In fact, $ğ”¼ï¼»âˆš{Z_n(1-Z_n)}ï¼½â‰ˆ2^{-Ïn}$.
	\pp
	This means that most of the times $âˆš{Z_n(1-Z_n)}$ is really small,	\\
	wherein either $Z_n$ is small or $1-Z_n$ is small.	\\
	Those that are not small are of measure $2^{-Ïn}$.
	\pp
	With more advanced tricks, $â„™ï½›Z_n(1-Z_n)â€  not smallâ€ ï½â‰ˆâ„“^{-0.49n}$;	\\
	this means $C-Râ‰ˆN^{-0.49}$.
}

\frame{{Polar counterpart of MDP}
	When $Z_n(1-Z_n)$ is small, martingale is controlled by the LDP theory.	\\
	When $Z_n(1-Z_n)$ is large, martingale is controlled by the CLT theory.
	\pp
	With positive measure, $Z_n$ goes back and forth between LDP and CLT.
	\pp
	With more advanced tricks, $Ã·{-\log P}{(C-R)^2}â‰ˆN^{0.99}.$
}

\frame{{Future works}
	Complexity is $O(N\log N)$. With more advanced tricks $O(N\log\log N)$.	\\\pause
	$âŸ¹$ Can it be better?
	\pp
	Polar codes apply to many-to-one communication,	\\
	one-to-many communication, and many others.	\\\pause
	$âŸ¹$ Generalize polar code to even more scenarios.
	\pp
	In some scenarios, random codes' best performance is still unclear.	\\\pause
	$âŸ¹$ Use polar to push random.
}

\frame{{Comment and questions}
	?
}

\appendix

\frame{{}
	\begin{tabular}{c@{\hskip2\tabcolsep}ccc@{\hskip2\tabcolsep}cc}
		\toprule
				&\multicolumn3c{\hskip-2\tabcolsep Symmetric}
				&\multicolumn2c{\hskip-1\tabcolsep Asymmetric}\\
				&binary			&prime-ary		&finite		&binary		&finite	\\
		\midrule
		LLN		&Arikan09		&STA09i			&STA09i		&SRDR12		&			\\
		LDPâ‹†	&AT09			&MT14			&Sasoglu11	&HY13		&			\\
		CLTâ‹†	&KMTU10,MHU16	&BGNRS18		&			&			&			\\
		MDPâ‹†	&GX15,MHU16		&BGS18			&			&			&			\\
		LDP		&KSU10,HMTU13	&				&			&			&			\\
		CLT		&FHMV18,GRY20	&				&			&			&			\\
		MDP		&				&				&			&			&			\\
		\bottomrule
	\end{tabular}
}

\end{document}

\frame{{Beamer and handout available at}
	\centering
	\begin{pspicture}(2in,2in)
		\psbarcode{https://ntu.symbol.codes/}{width=2.0 height=2.0}{qrcode}
	\end{pspicture}
	
	\url{https://ntu.symbol.codes/}
}

\end{document}



